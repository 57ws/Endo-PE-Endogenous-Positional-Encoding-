# Endo-PE-Endogenous-Positional-Encoding-
Make the model feel the location information naturally
# 基于动态生成与模型融合的位置编码方法研究


## 项目概述
位置信息是序列建模任务中的关键要素，传统位置编码（如正弦编码、可学习位置编码）多作为独立模块与输入特征拼接，存在灵活性不足、长序列适应性有限等问题。本项目提出两类新型位置编码方法：基于GRU的位置编码与基于逻辑掩码的位置编码，通过将位置信息生成机制融入模型本体（类似残差块），实现位置信息的动态感知与多层次传递，并验证了其在多任务场景下的有效性。


## 核心思想

### 1. 位置编码残差块
我们将位置编码不再是作为独立的加法项，而是通过残差模块嵌入到模型中，允许存在多个位置编码，使位置信息能多层次、自然地融入网络的前向传播过程中。

### 2. 动态生成位置编码
传统位置编码（如正弦编码）是预设的或预训练的，而本研究提出的GRU-PE和Logical-PE是动态生成的，能根据输入数据实时调整位置表示，更具灵活性和上下文适应性。

### 3. 双向 vs 单向位置感知
- GRU-PE：双向GRU结构，能捕捉前后文位置信息。
- Logical-PE：基于上三角掩码的单向注意力，模拟RNN的逐步感知机制，适合时序敏感任务。

### 4. 融合策略
我们探索了混合编码（如 sin_gru, sin_Logical），将静态正弦编码与动态编码结合，以兼顾训练稳定性和位置感知能力。

## 方法设计

### 1. GRU位置编码
利用双向GRU对序列的时序建模能力，将其输出作为位置信息载体，通过全连接层映射后与输入特征残差融合：
```python
class GRUPositionalEncoding(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.gru = nn.GRU(d_model, d_model // 2, batch_first=True, bidirectional=True)
        self.dense = nn.Linear(d_model, d_model)
    def forward(self, x):
        gru_out, _ = self.gru(x)  # 提取时序位置特征
        dense_out = self.dense(gru_out)
        return x + dense_out  # 位置信息注入
```

### 2. 逻辑掩码位置编码
基于Transformer解码器的上三角逻辑掩码（阻止当前时步受未来信息影响），通过带掩码的自注意力生成位置信息，实现类似于RNN的时序位置标注：
```python
class LogicalMaskEncoding(nn.Module):
    def __init__(self, d_model, nhead=4, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.register_buffer("mask", torch.triu(torch.ones(max_len, max_len), diagonal=1).bool())  # 上三角掩码
    def forward(self, x):
        attn_mask = self.mask[:x.size(1), :x.size(1)]
        x_attn, _ = self.self_attn(x, x, x, attn_mask=attn_mask)  # 带掩码的注意力生成位置信息
        y = self.norm1(x_attn)
        return x + y  # 位置信息注入
```

### 4. 融合方案
- **sin_gru**：正弦编码与GRU位置编码串联
- **sin_Logical**：正弦编码与逻辑掩码位置编码串联


## 实验结果
实验在4个任务（两点距离计算(自定义任务)、CoNLL2003命名实体识别、IMDB文本分类、EEG脑电信号分类）中验证，以下为关键任务按F1分数（或核心指标）降序排列的结果：
> 由于逻辑掩码和GRU组参数多于无位置编码和正弦编码组，所有实验中无位置编码和正弦编码组会拥有多一层transformer_encoder_layer(相比实验组)以确保公平。

### 1. 两点距离计算（核心指标：MSE/R²，按R²降序）

合成数据集，在长度100的序列中标记两个点，计算两点之间距离。

| 位置编码方法       | 测试损失（Test Loss） | MSE      | R²（拟合优度） | 训练时间（s） |
|--------------------|-----------------------|----------|----------------|---------------|
| sin_Logical        | 5.9842                | 5.9842   | 0.9894         | 26.26         |
| Logical            | 11.5690               | 11.5690  | 0.9795         | 27.19         |
| sin_gru            | 20.6170               | 20.6170  | 0.9634         | 12.60         |
| gru                | 22.0045               | 22.0045  | 0.9609         | 10.28         |
| sinusoidal         | 50.8588               | 50.8588  | 0.9097         | 13.32         |
| none（无位置编码） | 565.0877              | 565.0877 | -0.0037        | 11.04         |

> 无位置编码组拟合优度约为0，说明实验的有效性，证明了GRU和逻辑掩码确实能够提供位置信息。

### 2. CoNLL2003命名实体识别（F1排序）
| 位置编码方法       | 精确率（Precision） | 召回率（Recall） | F1分数 | 训练时间（s） |
|--------------------|---------------------|------------------|--------|---------------|
| gru                | 0.6897              | 0.6328           | 0.6600 | 22.49         |
| sin_gru            | 0.6704              | 0.6043           | 0.6356 | 25.37         |
| sin_Logical        | 0.6153              | 0.6034           | 0.6093 | 34.63         |
| sinusoidal         | 0.6124              | 0.5744           | 0.5928 | 37.58         |
| Logical            | 0.5518              | 0.5503           | 0.5511 | 38.62         |
| none（无位置编码） | 0.5245              | 0.4639           | 0.4923 | 30.22         |

> GRU组表现优秀，逻辑掩码组融合正弦编码后表现优良。

### 3. IMDB文本二分类（F1排序）
| 位置编码方法       | 准确率（Accuracy） | 精确率（Precision） | 召回率（Recall） | F1分数 | 训练时间（s） |
|--------------------|--------------------|---------------------|------------------|--------|---------------|
| sin_gru            | 0.8282             | 0.8088              | 0.8595           | 0.8334 | 79.49         |
| gru                | 0.8219             | 0.7998              | 0.8587           | 0.8282 | 80.03         |
| none（无位置编码） | 0.8053             | 0.8200              | 0.7822           | 0.8007 | 103.95        |
| sinusoidal         | 0.8069             | 0.8326              | 0.7682           | 0.7991 | 102.51        |
| sin_Logical        | 0.7998             | 0.8094              | 0.7843           | 0.7967 | 90.89         |
| Logical            | 0.7844             | 0.8129              | 0.7388           | 0.7741 | 103.69        |

> GRU组表现优秀，无位置编码优于其他(可能由于多一层encoder)，表明该任务对位置信息依赖低。

### 4. EEG脑电信号分类（F1排序）
| 位置编码方法       | 准确率（Accuracy） | 精确率（Precision） | 召回率（Recall） | F1分数 | 训练时间（s） |
|--------------------|--------------------|---------------------|------------------|--------|---------------|
| gru                | 0.8472             | 0.8304              | 0.8676           | 0.8486 | 113.50        |
| sin_gru            | 0.8477             | 0.8417              | 0.8514           | 0.8465 | 99.97         |
| Logical            | 0.8307             | 0.8131              | 0.8531           | 0.8326 | 163.83        |
| sin_Logical        | 0.7967             | 0.7770              | 0.8245           | 0.8001 | 164.41        |
| none（无位置编码） | 0.7993             | 0.8067              | 0.7801           | 0.7932 | 164.48        |
| sinusoidal         | 0.7799             | 0.7582              | 0.8135           | 0.7849 | 145.00        |

> GRU组仍效果优秀，逻辑编码组性能次优，无位置编码性能最差。

## 实验现象
1. **GRU编码优势显著**：在几乎所有任务中，GRU位置编码及其实验融合方案（sin_gru）表现最优，尤其在长序列（EEG）和时序敏感任务（距离计算）中，体现了强大的位置标注能力与训练效率（参数少、收敛快）。

2. **逻辑掩码需融合使用**：逻辑掩码单独作为位置编码时性能不稳定，但与正弦编码融合（sin_Logical）后，在距离计算任务中表现最佳，在命名实体识别中优于单一正弦编码，验证了融合互补的有效性。

3. **动态生成适配长序列**：GRU编码在长序列任务中优于正弦编码（如EEG），因其可动态生成任意长度位置信息，避免正弦编码对未训练长度的失效问题。

4. **任务依赖性**：在位置信息影响较弱的任务（如IMDB，无编码时准确率已达80%+），位置编码改进效果有限，印证了任务特性对位置编码需求的差异。

## 现象解释
1. **GRU提供的位置信息**： 由于每个时间步的隐藏状态都基于上一个时间步的状态，RNN可能建立了一个带有相对位置信息的序列，可能类似于递推报数。

2. **逻辑掩码提供的位置信息**： 由于每个时间步的输出都基于上前n个时间步，节点可能通过前面节点的信息，推导出自身的位置信息。

3. **位置之间的可区分性**： 不管是RNN还是逻辑掩码，都确保即便输入序列中有两个时间步完全相同，输出序列中也没有两个完全一样的时间步，符合位置编码原理。

4. **融合位置编码的性能提升**： 不同位置编码带有的位置信息可能性质不同，正弦编码的位置信息稳定，RNN位置编码则根据序列动态生成，可能实现功能互补。


## 未来展望
1. 探索多类型位置编码的深层融合机制（如多层级残差块嵌入）。
2. 优化逻辑掩码的双向性（如对称掩码设计），提升其与双向GRU的竞争力。
3. 在更长序列任务（如长文档理解）中验证方法的扩展性。

---

```
https://github.com/liluoyi666/Endo-PE-Endogenous-Positional-Encoding-.git

实验设备：RTX4070-8G

Endo-PE-Endogenous-Positional-Encoding-/
├── test.py         # 基础测试
├── imdb.py         # imdb测试
├── EEG.py          # EEG测试
├── CoNL2003.py     # CoNL2003测试
├── LICENSE
├── README.md
└── log/            #实验结果
```
